{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Well-being and Nutrition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we would like to study the link between well-being and nutrition.\n",
    "\n",
    "More precisely, our goal is to analyze if it is possible to predict the food consumption of an area from publicly available well-being data. We will study results at the level of London city wards, as this is the highest granularity we could find for general well-being data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true"
   },
   "source": [
    "## Context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From [Tesco Grocery 1.0, a large-scale dataset of grocery purchases in London](https://www.nature.com/articles/s41597-020-0397-7) paper, we were able to understand how the nutritional data of the average product of an area could help to predict the prevalence of common diseases such as diabetes. We were interested in the relationship between nutrition and general well-being indicators. Data on general well-being is publically available for any big city. On the other hand, nutritional purchases data has to be anonymized and is distributed among different private companies such as Tesco. We therefore aim to predict the nutritional information of the average product per area based on well-being measures.\n",
    "\n",
    "The City of London has conducted many studies and surveys on the well-being of its inhabitants, so it was not too difficult to find the required datasets to follow our studies. However, it would have been preferable to work with datasets at a higher granularity (LSOA or MSOA), as some phenomena might be smoothened out by bigger areas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true"
   },
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our study will use two datasets:\n",
    "\n",
    "1) The data provided by the Tesco Grocery 1.0 paper (`year_osward_grocery`) giving the nutritional informations of the average product per ward, the representativeness of Tesco's data and some additional information such as population, gender, and age.\n",
    "\n",
    "2) The well-being probability score dataset from [London datastore](https://data.london.gov.uk/). This data is from 2013. However, we can make the assumption that the well-being values do not significantly change in two years. In this file, they compute the total well-being per area in 2013 based on different categories (Health, Education, financial aspects, etc.). The spreadsheet is interactive, and therefore allows to weigh each category differently to compute the final well-being score. We decided to put the same weight to every category except the subjective well-being (self-stated happiness feeling). We set it to zero, as we want to make predictions based on objective and measurable input. The well-being Index Score (which we will rename to Total Well-being Score) is thus a mean of all the objective variables.\n",
    "\n",
    "You can find the well-being dataframe in the `data` folder (`./data/london-ward-well-being-probability-scores.xls`). We used the \"score\" page which gives the scores of the different variables for each area.\n",
    "\n",
    "The scores for each feature and each area are calculated as follows: $score = \\frac{data(area) - data(England \\, and \\, Wales)}{Standard \\, Deviation}$\n",
    "\n",
    "Therefore, if the score is positive, it means that the well-being variable is higher than UK's mean value for this area."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We provide a brief description of the fields you will encounter:\n",
    "\n",
    "- `area_id`: the number for each area of London at ward scale.\n",
    "\n",
    "- `Life_Expectancy`: Index scores were reversed so higher life expectancy equals better well-being. Source: ONS mortality data and GLA population projections, GLA Calculations.\n",
    "\n",
    "- `Childhood Obesity`: Children with a BMI greater than or equal to the 95th centile of the British 1990 growth reference (UK90) BMI distribution have been classified as obese. Source: National Obesity Observatory.\t\t\n",
    "\n",
    "- `Incapacity Benefit rate`: Incapacity Benefit (IB) is paid to people who are incapable of work and who meet certain contribution conditions. Severe Disablement Allowance (SDA) is paid to those unable to work for 28 weeks in a row or more because of illness or disability. Source: IB/SDA from DWP, Population from GLA projections.\n",
    "\n",
    "- `Unemployement rate`: Percentage of working-age residents claiming Jobseeker's Allowance (JSA) or National Insurance Credits. JSA is a benefit payable to unemployed people. In general, to be entitled to claim a person must be available for work, be actively seeking work, and have entered into a Jobseeker's Agreement with Jobcentre Plus. Source: JSA from DWP, Population from GLA projections.\n",
    "\n",
    "- `Crime rate - index`: Index scores of overall notifiable offences per 1,000 daytime population. Source: MPS, Home Office, and ONS Workday population 2011 Census.\n",
    "\n",
    "- `Deliberate fires`: Rate of all Deliberate Fire incidents (arson) recorded by the London Fire & Emergency Planning Authority per 1,000 population. Source: LFEPA via LASS team at GLA, and Population from GLA 2012 projections.\n",
    "\n",
    "- `Average Capped GCSE and Equivalent Point Score Per Pupil`: GCSE and Equivalent point scores for pupils at the end of Key Stage 4 (KS4) in maintained schools (Referenced by Location of Pupil Residence). Ward data calculated by apportioning Lower Layer Super Output Area (LSOA) data. Index scores were reversed so higher GCSE scores equals better well-being. Source: DfE (on Neighbourhood Statistics).\n",
    "\n",
    "- `Unauthorised Absence in All Schools (%)`: Pupil Absence in all maintained schools (Referenced by Location of Pupil Residence). Unauthorised Absence is absence without permission from a teacher or other authorised representative of the school. Ward data calculated by apportioning Lower Layer Super Output Area (LSOA) data. Source: DfE (on Neighbourhood Statistics).\n",
    "\n",
    "- `Dependent children in out-of-work families`: Data represent the number of children dependent on a parent or guardian who is claiming one or a combination of out-of-work benefits. Source: DWP and GLA 2012 population projections.\n",
    "\n",
    "- `Public Transport Accessibility`: take into account walk access time and service availability. The method is essentially a way of measuring the density of the public transport network at any location within Greater London. Each area was given an average score out of 8, where 8 is the highest level of accessibility. Open space was removed from the data as no population lives there. (Source: PTAL contours from Transport for London, further calculations by [GLA](http://www.tfl.gov.uk/businessandpartners/syndication/16493.aspx))\n",
    "\n",
    "- `Homes with access to open space & nature, and % greenspace`:  There are four types of public open space according to the 2011 London Plan. Homes further away than the maximum recommended distance are considered to be deficient in access to that type of public open space. Access to nature measures the proportion of homes with good access to nature. The final measure is the proportion of area that is greenspace within the ward. In these combined scores, each of the three measures has been given a weight of 33%. (Source: Greenspace Information for Greater London, and Ordnance Survey)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pprint\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "from sklearn.model_selection import cross_validate, train_test_split, cross_val_predict\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from statsmodels.stats import diagnostic\n",
    "from statsmodels.stats import diagnostic\n",
    "from statsmodels.stats.weightstats import ttost_ind\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "\n",
    "from utils import calculate_nutripoints\n",
    "\n",
    "pp = pprint.PrettyPrinter(depth=4)\n",
    "np.random.seed(42)\n",
    "\n",
    "sns.set_theme('notebook')\n",
    "sns.color_palette(\"colorblind\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true"
   },
   "source": [
    "## I) Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_grocery = pd.read_csv(\"data/year_osward_grocery.csv\")\n",
    "display(year_grocery.head())\n",
    "\n",
    "# for wellbeing_score, we have to get the scores of the individual variable in the \"Scores\" sheet\n",
    "# and the final score, which is the mean of all the variables, in the \"Ranked\" sheet\n",
    "\n",
    "wellbeing_scores = pd.read_excel(\n",
    "    \"data/london-ward-well-being-probability-scores.xls\", sheet_name=\"Scores\", header=[0, 1])\n",
    "display(wellbeing_scores.head())\n",
    "\n",
    "wellbeing_total_scores = pd.read_excel(\n",
    "    \"data/london-ward-well-being-probability-scores.xls\", sheet_name=\"Ranked\", header=[3], usecols=\"B:C\")\n",
    "display(wellbeing_total_scores.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true"
   },
   "source": [
    "## II) Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A) Filter data\n",
    "\n",
    "We consider only 80% of the areas, the ones with the highest representativeness. This will probably allow us to have more interesting and robust results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PERCENTAGE_SPLIT_REPRESENTATIVENESS = 0.8\n",
    "N = len(year_grocery)\n",
    "\n",
    "year_grocery = year_grocery.nlargest(\n",
    "    int(PERCENTAGE_SPLIT_REPRESENTATIVENESS * N), 'representativeness_norm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B) Compute Nutripoints\n",
    "\n",
    "To predict the nutritional variables of the average product per area, we thought it would be easier to assess a single variable representative of the individual ones per nutrients. We called it **Nutripoints**. \n",
    "\n",
    "We based our computation of the Nutripoints on the definition of the French national Nutri-Score. The bigger it is, the worse is the average product regarding the level of sugar, saturate, sodium and total energy it has. On the other hand, if it is negative (which is almost impossible), it represents products with higher level of proteins and fibers than \"bad nutrients\" (salt, sugar, saturate). We adapted the different ranges of the official formula to our database as we have average products and therefore a lower variance (small interval). You can see the detailed function in the file: `utils.py`\n",
    "\n",
    "We then add a column with the computed Nutripoints to our `year_grocery` dataset, which is the one with the nutritional values, assessing the nutritional quality of the average product of each London ward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_grocery = year_grocery[['energy_tot', 'saturate', 'salt',\n",
    "                            'sugar', 'f_fruit_veg', 'fibre', 'protein']].min(axis=0)\n",
    "\n",
    "max_grocery = year_grocery[['energy_tot', 'saturate', 'salt',\n",
    "                            'sugar', 'f_fruit_veg', 'fibre', 'protein']].max(axis=0)\n",
    "\n",
    "year_grocery[\"nutripoints\"] = year_grocery.apply(\n",
    "    lambda row: calculate_nutripoints(row, min_grocery, max_grocery), axis=1)\n",
    "\n",
    "year_grocery[\"nutripoints\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(year_grocery.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(year_grocery['area_id'].duplicated().any())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our Nutripoints are distributed from -1.5 to 15; having an interval of 16.5. However, as the mean is at 7.7 and the 1st and 3rd quartiles are at respectively 6 and 9.5, we see that most of the values are aggregated between these two values. The two outer quartiles have spread datapoints. The min and max values must be outliers.\n",
    "\n",
    "This will actually complicate our study, as there might not be many differences or at least considerable differences between the areas. It is probably because we have the average product per area.\n",
    "\n",
    "[//]: # \"Indeed, London's inhabitants buy mainly the same type of foods but not the same quality, according to their budget.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  C) Filter wellbeing data\n",
    "\n",
    "We filter the columns in order to keep only the information from the last year of the dataset, 2013. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only keep the columns containing 2013 and the code and name of the ward\n",
    "\n",
    "wellbeing_scores = wellbeing_scores.loc[:, (slice(\n",
    "    None), [2013, \"New ward code\", \"Ward name\"])].dropna(how=\"all\")\n",
    "\n",
    "# We first had two levels of indexes, one with the years and the other with the type of variable\n",
    "# Now that we only keep 2013, we drop the second level of line indexes.\n",
    "wellbeing_scores = wellbeing_scores.droplevel(1, axis=1)\n",
    "\n",
    "display(wellbeing_scores.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wellbeing_scores_columns = wellbeing_scores.columns.values.tolist()\n",
    "print(wellbeing_scores_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D) Merge datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this step, we will be able to do some exploratory data analysis, understanding the link between well-being and nutrition. To do so, we first merge the two datasets keeping only the columns that are interesting to put in parallel.\n",
    "\n",
    "First of all, we merge the two well-being datasets in order to have the Total Well-being score (column `Index Score 2013`) with all its components used to compute it. \n",
    "\n",
    "Then, we select the energy for each nutrient, the entropy and the Nutripoints from `year_grocery`. These variables will be used to represent the nutritional information of the Nutrition dataset. \n",
    "\n",
    "It is important to check for null values to know if our dataset is complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging the two wellbeing datasets according to the ward name in order to have the total score in only one wellbeing dataset\n",
    "wellbeing_scores = pd.merge(\n",
    "    left=wellbeing_scores, right=wellbeing_total_scores, left_on='Ward name', right_on=\"Ward\")\n",
    "\n",
    "wellbeing_scores = wellbeing_scores.drop(\"Ward\", axis=1)\n",
    "\n",
    "display(wellbeing_scores.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RENAMED_COLUMNS = {\n",
    "    'Incapacity Benefit rate': 'Incapacity Benefit Rate',\n",
    "    'Unemployment rate': 'Unemployment Rate',\n",
    "    'Average Capped GCSE and Equivalent Point Score Per Pupil': 'Average GCSE',\n",
    "    'Unauthorised Absence in All Schools (%)': 'Unauthorised Absence',\n",
    "    'Dependent children in out-of-work families': 'Dependent Children',\n",
    "    'Public Transport Accessibility': 'Public Transport Access',\n",
    "    'Homes with access to open space & nature, and % greenspace': 'Access to Nature',\n",
    "    'Subjective well-being average score': 'Happiness Score',\n",
    "    'Index Score 2013': 'Wellbeing Score',\n",
    "    'Crime rate - Index': 'Crime Rate'\n",
    "}\n",
    "\n",
    "wellbeing_scores = wellbeing_scores.rename(\n",
    "    columns=RENAMED_COLUMNS, errors=\"raise\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Selection of the columns of interest in year_grocery dataset\n",
    "list_column = [\"area_id\", \"energy_tot\", \"energy_fat\", \"energy_saturate\", \"energy_sugar\", \"energy_protein\",\n",
    "               \"energy_carb\", \"energy_fibre\", \"energy_alcohol\", \"h_nutrients_calories\", \"nutripoints\"]\n",
    "\n",
    "year_grocery = year_grocery.loc[:,\n",
    "                                year_grocery.columns.isin(list(list_column))]\n",
    "display(year_grocery.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging the 2 datasets of interest: year grocery and wellbeing scores according to the ward name\n",
    "wellbeing_grocery = pd.merge(\n",
    "    left=year_grocery, right=wellbeing_scores, left_on='area_id', right_on=\"New ward code\")\n",
    "\n",
    "wellbeing_grocery = wellbeing_grocery.drop(\"New ward code\", axis=1)\n",
    "display(wellbeing_grocery.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for any null values in the new dataset\n",
    "wellbeing_grocery_columns = wellbeing_grocery.columns.values.tolist()\n",
    "wellbeing_grocery.isnull().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dataset of study is complete as there is no null values in any of our columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Understanding better how each variable is organized: its min, max, mean, standard deviation and quartiles\n",
    "wellbeing_grocery.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our analysis, we will create a database from our merged one (`wellbeing_grocery`) keeping only the numerical variables. We also standardise the values in order to be able to compare them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of columns of interest in the wellbeing dataset\n",
    "COLUMNS_SCORES = [\n",
    "    'Life Expectancy',\n",
    "    'Childhood Obesity',\n",
    "    'Incapacity Benefit Rate',\n",
    "    'Unemployment Rate',\n",
    "    'Crime Rate',\n",
    "    'Deliberate Fires',\n",
    "    'Average GCSE',\n",
    "    'Unauthorised Absence',\n",
    "    'Dependent Children',\n",
    "    'Public Transport Access',\n",
    "    'Access to Nature',\n",
    "    'Happiness Score',\n",
    "    'Wellbeing Score'\n",
    "]\n",
    "\n",
    "# List of columns of interest in the nutritional dataset\n",
    "COLUMNS_GROCERY = [\n",
    "    'energy_fat',\n",
    "    'energy_saturate',\n",
    "    'energy_sugar',\n",
    "    'energy_protein',\n",
    "    'energy_carb',\n",
    "    'energy_fibre',\n",
    "    'energy_alcohol',\n",
    "    'energy_tot',\n",
    "    'h_nutrients_calories',\n",
    "    'nutripoints'\n",
    "]\n",
    "\n",
    "COLUMNS = COLUMNS_GROCERY + COLUMNS_SCORES\n",
    "\n",
    "# Selection of the numerical columns of interest in the wellbeing_grocery dataset\n",
    "wellbeing_grocery_analysis = wellbeing_grocery[COLUMNS].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardizing features by removing the mean and scaling to unit variance\n",
    "scaler = StandardScaler()\n",
    "wellbeing_grocery_analysis[wellbeing_grocery_analysis.columns] = scaler.fit_transform(wellbeing_grocery_analysis\n",
    "                                                                                      [wellbeing_grocery_analysis.columns])\n",
    "wellbeing_grocery_analysis.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true"
   },
   "source": [
    "## III) Exploratory Data Analysis\n",
    "\n",
    "In this part, we will do some tests putting in parallel the well-being variables and the nutritional ones to analyse if they are correlated and therefore if we could really use one to predict the other. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A) Repartition of the values for each variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explanatory data analysis: visually show the distribution of the values for each variable thanks to boxplot\n",
    "fig, ax = plt.subplots(4, 6, figsize=(16, 8), sharey=False)\n",
    "\n",
    "for i in range(len(COLUMNS)):\n",
    "    sbplt = ax[int(i/6), i % 6]\n",
    "\n",
    "    sns.boxplot(data=wellbeing_grocery_analysis.iloc[:, i], ax=sbplt)\n",
    "    sbplt.set_xlabel('')\n",
    "    sbplt.set_ylabel('')\n",
    "    sbplt.set_title(COLUMNS[i], loc='center', wrap=True)\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.subplots_adjust(top=0.9)\n",
    "\n",
    "fig.suptitle('Boxplot for each wellbeing and nutritional variable', fontsize=18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These box plots give us a general view of the distribution of each variable. Since we would like to cluster nutrient data for specific areas and predict nutritional habits according to well-being characteristics, it is important to observe differences between areas for the various features. Here, the nutritional as well as the well-being features seem to take many values depending on the ward. Therefore, it is meaningful to understand if these differences in the nutritional variables are linked to differences in the well-being features. Moreover, some outliers can sometimes be seen, for example for the energy_saturate and the crime rate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploratory data analysis: distribution of the different variables by counting the number of observations per value\n",
    "fig, ax = plt.subplots(4, 6, figsize=(16, 8), sharey=False)\n",
    "\n",
    "for i in range(len(COLUMNS)):\n",
    "    sbplt = ax[int(i / 6), i % 6]\n",
    "\n",
    "    sns.histplot(wellbeing_grocery_analysis.iloc[:, i], ax=sbplt)\n",
    "    sbplt.set_xlabel('')\n",
    "    sbplt.set_ylabel('')\n",
    "    sbplt.set_title(COLUMNS[i], wrap=True)\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.subplots_adjust(top=0.9)\n",
    "\n",
    "fig.suptitle('Distributions of separate features', fontsize=18);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that some variables, as \"Access to nature\" and \"Life Expectancy\", follow a normal distribution. Let's check our assumption!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explanatory data analysis: distribution of the different variables by computing the proportion of values below or equal\n",
    "# to a certain value\n",
    "fig, ax = plt.subplots(4, 6, figsize=(16, 8), sharey=False)\n",
    "\n",
    "for i in range(len(COLUMNS)):\n",
    "    sbplt = ax[int(i / 6), i % 6]\n",
    "\n",
    "    sns.ecdfplot(wellbeing_grocery_analysis.iloc[:, i], ax=sbplt)\n",
    "    sbplt.set_xlabel('')\n",
    "    sbplt.set_ylabel('')\n",
    "    sbplt.set_title(COLUMNS[i], wrap=True)\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.subplots_adjust(top=0.9)\n",
    "\n",
    "fig.suptitle('CDF of separate features', fontsize=18);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe districumulative distribution function of the format of a normal distribution, it seems that certain variables follow this kind of distribution.\n",
    "\n",
    "We will now test it more thoroughly using three different tests to check normality. We set the significance level to $\\alpha = 0.05$, and only accept the columns that pass every test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for any normal distribution\n",
    "from collections import defaultdict\n",
    "from scipy.stats import shapiro, normaltest\n",
    "alpha = 0.05\n",
    "\n",
    "\n",
    "results = defaultdict(list)\n",
    "# normality test\n",
    "for col in COLUMNS:\n",
    "    data = wellbeing_grocery_analysis[col]\n",
    "    stat_shap, pvalue_shap = shapiro(data)\n",
    "    stat_normal, pvalue_normal = normaltest(data)\n",
    "    stat_kstest, pvalue_kstest = diagnostic.kstest_normal(data, dist='norm')\n",
    "    passed_tests = np.sum(\n",
    "        np.array([pvalue_shap, pvalue_normal, pvalue_kstest]) > alpha)\n",
    "    if passed_tests:\n",
    "        # failed to reject H0\n",
    "        results['Factor'].append(col)\n",
    "        results['Passed Tests'].append(passed_tests)\n",
    "\n",
    "display(pd.DataFrame(results).sort_values(\"Passed Tests\", ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second value returned by the function is the p-value. When the p_value < 0.05 -> we can reject the null hypothesis that the data comes from a normal distribution!\n",
    "Here, we observe that:\n",
    "\n",
    "- energy_carb\n",
    "- energy_tot\n",
    "- Life Expectancy\n",
    "- Access to Nature\n",
    "- Well-being Score\n",
    "\n",
    "have a p-value > 0.05 for all normality test.\n",
    "\n",
    "Therefore, the null hypothesis is not rejected and we can say that those variables follow a normal distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B) Correlation study\n",
    "\n",
    "After checking how each variable is distributed: its interval, mean and standard deviation, we will now observe the correlation between our Nutripoints, representative of nutritional informations, and each well-being variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing and plotting the correlation between the well-being scores and nutripoints\n",
    "correlation = wellbeing_grocery_analysis[COLUMNS_SCORES +\n",
    "                                         ['nutripoints']].corr()\n",
    "correlation.to_pickle(\"plot_data/wellbeing_correlation.pkl\")\n",
    "\n",
    "fig = plt.figure(figsize=(10, 5))\n",
    "\n",
    "sns.heatmap(correlation);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This heatmap helps us visualize the correlation between the different variables. If we focus on the correlation between Nutripoints and wellbeing features (last row), we can see actually that the values are quite low because the color is mainly purple corresponding to the negative correlation values and they are all below 0.4 which is not high. This is not a good augur for the continuation of the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing the correlation between the different variables from the dataset according to the spearman method\n",
    "\n",
    "correlation = wellbeing_grocery_analysis.corr(method=\"spearman\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the results obtained thanks to the heatmap, we decided to analyze if it was only the nutripoints that did not have a high correlation with the well-being variables or if we could also observe this phenomenom for the other nutritional variables (energy and entropy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the correlation between the nutritional and the well-being variables\n",
    "\n",
    "fig, ax = plt.subplots(5, 2, figsize=(15, 10), sharex=True, sharey=True)\n",
    "\n",
    "for i in range(len(COLUMNS_GROCERY)):\n",
    "    sbplt = ax[int(i / 2), i % 2]\n",
    "\n",
    "    correlation[COLUMNS_GROCERY[i]][COLUMNS_SCORES].plot.bar(\n",
    "        x=None, y=None, width=0.8, legend=None, ax=sbplt)\n",
    "    sbplt.set_title(wellbeing_grocery_analysis.columns[i], wrap=True)\n",
    "\n",
    "\n",
    "fig.text(0.08, 0.5, 'Spearman R', va='center', rotation='vertical')\n",
    "fig.subplots_adjust(top=0.9)\n",
    "\n",
    "fig.suptitle(\"Spearman Correlation\", fontsize=18);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe on the graphs above that the correlation is low for all the nutritional variables. They never exceed 0.5 except for energy_fibre. Therefore, it seems that it is going to be difficult to predict Nutripoints from the well-being variables because they only have a few impact on the nutritional aspect of an area. Especially the Happiness Score is hardly correlated to any nutritional value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C) Regression analysis\n",
    "\n",
    "We have seen that well-being variables are not very correlated to the nutritional ones. However, to be sure about it, we want to test some regressions and observe the R-squared and p-values for each coefficient.\n",
    "\n",
    "We will try predicting entropy and fibre (the nutritional values with the highest correlation), with the well-being variables that seem, in our opinion, the most linked to nutrition.\n",
    "\n",
    "Indeed, we could think that if unemployement is high, people have less money to spend on healthy food. Furthermore, childhood obesity must be linked to unhealthy food, high in fat and sugar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLUMNS_CORRELATED = [\"Life Expectancy\", \"Incapacity Benefit Rate\",\n",
    "                      \"Unemployment Rate\", \"Crime Rate\", \"Childhood Obesity\", \"Access to Nature\"]\n",
    "\n",
    "# Selection of the wellbeing features that seem most correlated to nutrition\n",
    "reg_features = 'Q(\"Life Expectancy\") + Q(\"Incapacity Benefit Rate\") + Q(\"Unemployment Rate\") + Q(\"Crime Rate\") \\\n",
    "+ Q(\"Childhood Obesity\") + Q(\"Access to Nature\")'\n",
    "\n",
    "# Linear regression to predict entropy from those features\n",
    "mod = smf.ols(formula='h_nutrients_calories ~ ' +\n",
    "              reg_features, data=wellbeing_grocery_analysis)\n",
    "res = mod.fit()\n",
    "print(res.summary());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear regression to predict energy_fibre\n",
    "mod = smf.ols(formula='energy_fibre ~ ' + reg_features,\n",
    "              data=wellbeing_grocery_analysis)\n",
    "res = mod.fit()\n",
    "print(res.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "R-squared are quite low meaning that either 20% or 40% of the data can be predicted. Furthermore, the p-values of the the coefficients should be below 0.05 in order to be meaningful, which they are not, unless for childhood obesity when predicting the entropy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true"
   },
   "source": [
    "### D) PCA\n",
    "\n",
    "Now, we do a PCA to project the well-being data into a 2D plane."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# PCA on the nutrition features\n",
    "\n",
    "wellbeing_grocery_analysis = wellbeing_grocery[COLUMNS_GROCERY]\\\n",
    "    .dropna().copy()\n",
    "\n",
    "wellbeing_grocery_reduced_pca = PCA(n_components=2).fit_transform(\n",
    "    wellbeing_grocery_analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labels computed as explained above\n",
    "labels = wellbeing_grocery.apply(\n",
    "    lambda row: \"g\" if row['Wellbeing Score'] >= 0 else \"r\", axis=1)\n",
    "\n",
    "# Plot the data reduced to a 2D plane with PCA\n",
    "plt.figure(figsize=(14, 5))\n",
    "plt.scatter(wellbeing_grocery_reduced_pca[:, 0],\n",
    "            wellbeing_grocery_reduced_pca[:, 1], c=labels, alpha=0.6)\n",
    "plt.xlabel('component 1')\n",
    "plt.ylabel('component 2')\n",
    "\n",
    "plt.title(\"PCA projection of nutritional values\", fontsize=18);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Election of the column we would like to determine the number of clusters for: here it is the Nutripoints\n",
    "year_grocery = year_grocery.sort_values(by=[\"nutripoints\"])\n",
    "\n",
    "columns_kmeans = ['nutripoints']\n",
    "year_grocery_kmeans = year_grocery[columns_kmeans].copy()\n",
    "\n",
    "# Function to plot the sse\n",
    "\n",
    "\n",
    "def plot_sse(X, start=2, end=11):\n",
    "    sse = []\n",
    "    for k in range(start, end):\n",
    "        # Assign the labels to the clusters\n",
    "        kmeans = KMeans(n_clusters=k, random_state=10).fit(X)\n",
    "        sse.append({\"k\": k, \"sse\": kmeans.inertia_})\n",
    "    sse = pd.DataFrame(sse)\n",
    "    # Plot the data\n",
    "    plt.plot(sse.k, sse.sse)\n",
    "    plt.xlabel(\"K\")\n",
    "    plt.ylabel(\"Sum of Squared Errors\")\n",
    "    plt.title(\"Sum of Squared Errors curve to find the optimal k\", fontsize=13)\n",
    "\n",
    "\n",
    "plot_sse(year_grocery_kmeans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E) k-Means Clustering to Label Data\n",
    "\n",
    "In order to better understand how nutripoints are spread and to see if we can identify some clusters linked to well-being features, we will process the data using K-means."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know that a positive value corresponds to a better score than the England average whereas the negative are worse. We could then label our datapoints with respect to their wellbeing score (value comptued from all well-being variables, higher corresponds to a better well-being estimate). We set the color of positive well-being scores to green, negative to red. Unfortunately, the nutritional values do not seem create clusters or visually separate depending on the well-being score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now label the data based on its nutripoints. We apply this to `year_grocery` as we want to take the distribution for all the wards and not only the ones for which we also have wellbeing values.\n",
    "\n",
    "To understand how many clusters we can determine for the nutripoints, we first plot the sum of squared erros curve for each number of cluster in order to find the optimal number k. It is called the elbow method. Indeed, the point where the curve actually breaks should be the best number of clusters for this dataset. \n",
    "To confirm our choice, we will also do the silhouette score. This curve represents the score assessed for each number of clusters considering the separation distance between the resulting clusters. A peak on the curve indicates that the score reached its global maximum at the optimal k. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2nd method to choose k: the silhouette score\n",
    "silhouettes = []\n",
    "\n",
    "# Trying for k between 2 and 10\n",
    "for k in range(2, 11):\n",
    "    # Cluster the data and assign the labels\n",
    "    labels = KMeans(n_clusters=k, random_state=10).fit_predict(\n",
    "        year_grocery_kmeans)\n",
    "    # Get the Silhouette score\n",
    "    score = silhouette_score(year_grocery_kmeans, labels)\n",
    "    silhouettes.append({\"k\": k, \"score\": score})\n",
    "\n",
    "# Convert to dataframe\n",
    "silhouettes = pd.DataFrame(silhouettes)\n",
    "\n",
    "# Plot the data\n",
    "plt.plot(silhouettes.k, silhouettes.score)\n",
    "plt.xlabel(\"K\")\n",
    "plt.ylabel(\"Silhouette score\")\n",
    "plt.title(\"Silhouette score curve to find the optimal k\", fontsize=13);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the sse plot, we can observe an elbow for k=3 and k=5. On the other hand, when plotting the silhouette score, we see that the curve goes down for k=3 but the silhouette score is higher for k=5. Therefore, we decided to divide the nutripoints into 5 clusters. To do so, we use the K-means method. We also rank the areas from the lowest nutripoints to the biggest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Clustering the data with the current number of clusters\n",
    "kmean = KMeans(n_clusters=5, random_state=42).fit(year_grocery_kmeans)\n",
    "\n",
    "# sort labels according to center\n",
    "idx = np.argsort(kmean.cluster_centers_.sum(axis=1))\n",
    "lut = np.zeros_like(idx)\n",
    "lut[idx] = np.arange(5)\n",
    "\n",
    "# Plotting the data by using the labels as color\n",
    "year_grocery['nutri_class'] = lut[kmean.labels_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(year_grocery.head())\n",
    "print(year_grocery.shape)\n",
    "print(wellbeing_grocery.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_grocery.to_pickle(\"data/grocery_nutripoints.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `year_grocery` database with the Nutripoints and the class they are assigned to will be saved into pickle in order to be reused in another file such as  `Nutripoints` where we analyse in more details the distribution of the different clusters and the notebook `fastfood` where we want to study another correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging the 2 datasets of interest: year grocery and wellbeing scores according to the ward name\n",
    "wellbeing_grocery_pca = pd.merge(\n",
    "    left=year_grocery, right=wellbeing_scores, left_on='area_id', right_on=\"New ward code\")\n",
    "\n",
    "wellbeing_grocery_pca = wellbeing_grocery_pca.drop(\"New ward code\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA on the wellbeing features\n",
    "\n",
    "wellbeing_grocery_analysis = wellbeing_grocery_pca[COLUMNS_CORRELATED]\\\n",
    "    .dropna().copy()\n",
    "\n",
    "wellbeing_grocery_reduced_pca = PCA(n_components=2).fit_transform(\n",
    "    wellbeing_grocery_analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the data reduced to a 2D plane with PCA\n",
    "color = wellbeing_grocery_pca[\"nutri_class\"].values\n",
    "plt.figure(figsize=(14, 6))\n",
    "# plt.scatter(wellbeing_grocery_reduced_pca[:, 0],\n",
    "#            wellbeing_grocery_reduced_pca[:, 1], c=color, alpha=0.6)\n",
    "\n",
    "plt.scatter(wellbeing_grocery_reduced_pca[:, 0],\n",
    "            wellbeing_grocery_reduced_pca[:, 1],\n",
    "            c=color, edgecolor='none', alpha=0.5,\n",
    "            cmap=plt.cm.get_cmap('Paired', 5))\n",
    "plt.xlabel('component 1')\n",
    "plt.ylabel('component 2')\n",
    "plt.colorbar()\n",
    "\n",
    "plt.title(\"PCA projection of wellbeing values\", fontsize=18);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The different colors correspond to the nutri_class every datapoint is in. Again, the correlated features do not seem to be enough to separate the nutripoints into clear clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true"
   },
   "source": [
    "### F) Equivalence Test\n",
    "\n",
    "Our visual analysis suggests that there is no significant relationship between a general well-being score and nutritional quality of an area. To formalize this intuition, we will perform an equivalence test. Equivalence tests are a variation of hypothesis tests used to draw statistical inferences from observed data. In equivalence tests, the null hypothesis is defined as an effect large enough to be deemed interesting, specified by an equivalence bound. The alternative hypothesis is any effect that is less extreme than said equivalence bound. The observed data is statistically compared against the equivalence bounds.\n",
    "\n",
    "If the statistical test indicates the observed data is surprising, assuming that true effects at least as extreme as the equivalence bounds, a Neyman-Pearson approach to statistical inferences can be used to reject effect sizes larger than the equivalence bounds with a pre-specified Type 1 error rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_wellbeing(row, column):\n",
    "    if row[column] > 0:\n",
    "        return \"prosperous\"\n",
    "    else:\n",
    "        return \"unpropitious\"\n",
    "\n",
    "\n",
    "wellbeing_grocery[\"prosperity\"] = wellbeing_grocery.apply(\n",
    "    lambda row: classify_wellbeing(row, \"Wellbeing Score\"), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wellbeing_grocery[['nutripoints', 'prosperity']\n",
    "                  ].boxplot(by='prosperity', figsize=(10, 5))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming that the threshold value for a practical difference in nutripoint is 1, the null and alternative hypotheses would be:\n",
    "\n",
    "$H_0: |\\mu_1 - \\mu_2| \\geq 1$\n",
    "\n",
    "$H_1: |\\mu_1 - \\mu_2| < 1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_values = []\n",
    "\n",
    "for col in COLUMNS_SCORES:\n",
    "    median_col = wellbeing_grocery[col].median()\n",
    "\n",
    "    if col in (\"Average GCSE\",\n",
    "               \"Public Transport Access\",\n",
    "               \"Access to Nature\",\n",
    "               \"Happiness Score\",\n",
    "               \"Wellbeing Score\"):\n",
    "        wellbeing_grocery[\"prosperity\"] = wellbeing_grocery.apply(\n",
    "            lambda row: classify_wellbeing(row, col), axis=1)\n",
    "    else:\n",
    "        wellbeing_grocery[\"prosperity\"] = wellbeing_grocery.apply(\n",
    "            lambda row: classify_wellbeing(-1 * row, col), axis=1)\n",
    "\n",
    "    treated = wellbeing_grocery[wellbeing_grocery['prosperity']\n",
    "                                == \"prosperous\"]['nutripoints']\n",
    "    control = wellbeing_grocery[wellbeing_grocery['prosperity']\n",
    "                                == \"unpropitious\"]['nutripoints']\n",
    "\n",
    "    p_value_non_equivalence, lower_threshold_test, upp_threshold_test = ttost_ind(\n",
    "        x1=control, x2=treated, low=-1, upp=1)\n",
    "\n",
    "    p_values.append(p_value_non_equivalence)\n",
    "\n",
    "\n",
    "# multiple tests correction\n",
    "rejected, p_values_corrected, _, _ = multipletests(p_values)\n",
    "\n",
    "for p_value_non_equivalence, col in zip(p_values_corrected, COLUMNS_SCORES):\n",
    "\n",
    "    test_result = \"H_0 rejected\" if p_value_non_equivalence < 0.05 else \"H_0 not rejected\"\n",
    "    print(f\"{test_result} for {col}, p-value: {p_value_non_equivalence:.2e}\")\n",
    "    # print(lower_threshold_test)\n",
    "    # print(upp_threshold_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in traditional hypothesis testing, the aim of our equivalence test is to reject the null hypothesis in order to conclude that the alternative hypothesis is demonstrated beyond reasonable doubt. In our case, if we can reject the null hypothesis $H_0$ (that the difference between the two groups is higher than or equal to 1), we are allowed to conclude that the mean difference between the groups at the population level is below 1, a value defined as trivial.\n",
    "The `treatment` is having a positive well-being score.\n",
    "\n",
    "However, we can not conclude that the effect of the experimental treatment is practically insignificant, as the null hypothesis is not rejected for any feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true"
   },
   "source": [
    "## IV) Predictive Models\n",
    "\n",
    "We have seen that there are some columns with a considerable correlation between well-being features and our computed nutripoints. Therefore, we will now try to predict the nutripoints from these well-being features and assess the quality of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will test 3 different models to see if we can can successfully predict the nutripoins from the well-being features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = wellbeing_grocery[COLUMNS_CORRELATED]\n",
    "y = wellbeing_grocery[[\"nutripoints\"]]\n",
    "\n",
    "scaler_X = StandardScaler()\n",
    "scaler_y = StandardScaler()\n",
    "\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.2)\n",
    "X_tr = X_tr.copy()\n",
    "X_te = X_te.copy()\n",
    "\n",
    "X_tr[X_tr.columns] = scaler_X.fit_transform(X_tr[X_tr.columns])\n",
    "X_te[X_te.columns] = scaler_X.transform(X_te[X_te.columns])\n",
    "\n",
    "y_tr = scaler_y.fit_transform(y_tr).ravel()\n",
    "y_te = scaler_y.transform(y_te).ravel()\n",
    "\n",
    "print(X_tr.shape, X_te.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We  will train a gradient boosting regressor. We decided to start with this model given our low amount of data samples. Ensemble methods like the gradient boosting regressor are known to be much more robust against over-fitting than simpler models like linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Gradient Boosting Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regression models\n",
    "models = {}\n",
    "\n",
    "score_mse = 'neg_mean_squared_error'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Life Expectancy, Unemployment Rate, Deliberate Fires, Public Transport Access and Happiness Score are the selected variables for prediction of Nutripoints. Some features meaningful for the Linear Regression such as Well-being Score and GCSE are not here and on the other hand Happiness Score was not significant in the previous regression. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Gradient Boosting Regression ##\n",
    "\n",
    "n_estim_list = np.arange(1, 101)\n",
    "\n",
    "mse_train_gradboost = []\n",
    "mse_val_gradboost = []\n",
    "\n",
    "for n_estim in n_estim_list:\n",
    "    gradboost = GradientBoostingRegressor(n_estimators=n_estim)\n",
    "\n",
    "    scores_gradboost = cross_validate(gradboost, X_tr, y_tr, cv=10, return_train_score=True,\n",
    "                                      scoring=(score_mse))\n",
    "\n",
    "    mse_train_gradboost.append(-np.mean(scores_gradboost['train_score']))\n",
    "    mse_val_gradboost.append(-np.mean(scores_gradboost['test_score']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = np.argmin(mse_val_gradboost)\n",
    "opt_param_gradboost = n_estim_list[ind]\n",
    "opt_mse_val_gradboost = mse_val_gradboost[ind]\n",
    "\n",
    "print(\"n =\", opt_param_gradboost)\n",
    "\n",
    "models[\"gradboost\"] = (opt_mse_val_gradboost, opt_param_gradboost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Ridge Regression ##\n",
    "\n",
    "alphas_list = np.linspace(start=0.1, stop=10, num=100)\n",
    "\n",
    "mse_train_ridge = []\n",
    "mse_val_ridge = []\n",
    "\n",
    "for a in alphas_list:\n",
    "    ridge = Ridge(alpha=a)\n",
    "\n",
    "    scores_ridge = cross_validate(ridge, X_tr, y_tr, cv=10, return_train_score=True,\n",
    "                                  scoring=(score_mse))\n",
    "\n",
    "    mse_train_ridge.append(-np.mean(scores_ridge['train_score']))\n",
    "    mse_val_ridge.append(-np.mean(scores_ridge['test_score']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = np.argmin(mse_val_ridge)\n",
    "opt_param_ridge = alphas_list[ind]\n",
    "opt_mse_val_ridge = mse_val_ridge[ind]\n",
    "\n",
    "print(\"alpha =\", opt_param_ridge)\n",
    "\n",
    "models[\"ridge\"] = (opt_mse_val_ridge, opt_param_ridge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Random Forest Regression ##\n",
    "\n",
    "mse_train_rf = []\n",
    "mse_val_rf = []\n",
    "\n",
    "for n_estim in n_estim_list:\n",
    "    rf = RandomForestRegressor(n_estimators=n_estim)\n",
    "\n",
    "    scores_rf = cross_validate(rf, X_tr, y_tr, cv=10, return_train_score=True,\n",
    "                               scoring=(score_mse))\n",
    "\n",
    "    mse_train_rf.append(-np.mean(scores_rf['train_score']))\n",
    "    mse_val_rf.append(-np.mean(scores_rf['test_score']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = np.argmin(mse_val_rf)\n",
    "opt_param_rf = n_estim_list[ind]\n",
    "opt_mse_val_rf = mse_val_rf[ind]\n",
    "\n",
    "print(\"n_estimators =\", opt_param_rf)\n",
    "models[\"rf\"] = (opt_mse_val_rf, opt_param_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 3, figsize=(20, 4), sharey=True)\n",
    "\n",
    "# GradBoost\n",
    "\n",
    "line_1 = ax[0].plot(n_estim_list, mse_train_gradboost, label='training error')\n",
    "line_2 = ax[0].plot(n_estim_list, mse_val_gradboost, label='validation error')\n",
    "ax[0].set_xlabel('Number of Estimators')\n",
    "ax[0].set_ylabel('MSE')\n",
    "ax[0].set_title('GradBoost Regressor')\n",
    "ax[0].legend()\n",
    "\n",
    "##############################################\n",
    "\n",
    "# Ridge\n",
    "\n",
    "line_1 = ax[1].plot(alphas_list, mse_train_ridge, label='training error')\n",
    "line_2 = ax[1].plot(alphas_list, mse_val_ridge, label='validation error')\n",
    "ax[1].set_xlabel('Alpha')\n",
    "ax[1].set_ylabel('MSE')\n",
    "ax[1].legend()\n",
    "ax[1].set_title('Ridge Regressor')\n",
    "\n",
    "\n",
    "##############################################\n",
    "\n",
    "# RandomForest\n",
    "\n",
    "# print(x_arr, mse_train)\n",
    "line_1 = ax[2].plot(n_estim_list, mse_train_rf, label='training error')\n",
    "line_2 = ax[2].plot(n_estim_list, mse_val_rf, label='validation error')\n",
    "ax[2].set_xlabel('Number of Estimators')\n",
    "ax[2].set_ylabel('MSE')\n",
    "ax[2].set_title('Random Forest Regressor')\n",
    "ax[2].legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model selection\n",
    "\n",
    "print(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results for the selected model\n",
    "\n",
    "gradboost = RandomForestRegressor(n_estimators=opt_param_rf)\n",
    "gradboost.fit(X_tr, y_tr)\n",
    "\n",
    "y_pred = gradboost.predict(X_te)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "ax.scatter(y_te, y_pred, edgecolors=(0, 0, 0))\n",
    "p1 = max(max(y_pred), max(y_te))\n",
    "p2 = min(min(y_pred), min(y_te))\n",
    "plt.plot([p1, p2], [p1, p2], 'b-', c='r')\n",
    "ax.set_xlabel('Original')\n",
    "ax.set_ylabel('Predicted')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We finally test our results on the testing set and plot the results. The dots should be aligned close to the red line if the predictions were accurate.\n",
    "\n",
    "We see that the best model's fit does not achieve a very good accuracy. Although the bias does not seem very high, the model does not achieve to grasp the variance of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true"
   },
   "source": [
    "## V) Conclusions\n",
    "\n",
    "We conclude that we were not able to successfully fit a generalizable model to predict the nutritional quality of an area from publicly available well-being data.\n",
    "\n",
    "Reasons for this could be:\n",
    "\n",
    "* Lack of training and test data.\n",
    "* The models we fitted are not appropriate to this task, or more thorough hyperparameter tuning needed.\n",
    "* Wards have a too high geographic granularity, smoothen out interesting effects.\n",
    "* There is no underlying effect between well-being factors and nutripoints (nutritional quality of an area), or effect is very small."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VI) Additional Analyses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true"
   },
   "source": [
    "### A) Nutripoints Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will try to better understand how the health score (nutripoints) of the average products are distributed through the areas in London, how certain nutritional variables can influence it and what is the average nutrients composition of the average product of all the areas. \n",
    "\n",
    "We have decided to create the **nutripoints** variable to facilitate the prediction. Indeed, it is easier to predict one variable than all the nutritional ones. It therefore represents the healthiness of the average product of a specific area by taking into account the different nutrients it is based on.  \n",
    "\n",
    "We based our computation of the nutripoints on the definition of the national french Nutri-Score. The bigger it is, the worst is the average product regarding the level of sugar, saturate, sodium and total energy it has. On the other hand, if it is negative (which is almost impossible), it represents products with higher level of proteins and fibers than \"bad nutrients\" (salt, sugar, saturate). We adapted the different ranges of the official formula to our database as we have average products and therefore very close data ranges. You can see the detailed function in the file: `utils.py`\n",
    "\n",
    "Moreover, we realised than when summing the main nutrients (carb, protein, fibre, fat, salt), we didn't reach either the average product's weight or 100. Therefore, we rescaled all our values for a 100g in order to be able to compare the different areas. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Imports\n",
    "\n",
    "We import `year_grocery` in order to obtain the weight of each nutrients and to compute the total weight for the typical London product analysis.\n",
    "\n",
    "We also use `wellbeing_grocery` from the `main` notebook where `nutripoints` and `nutri_class` are already computed. However, we select only the columns referring to nutrition variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually perceived as \"bad\" nutrients, salt and sugar are consumed excessively in the total proportion of intakes. On the other hand, saturate, also classified as an unhealthy nutrient is consumed reasonably with a value that doesn't exceed the recommendations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Imports\n",
    "\n",
    "year_grocery = pd.read_csv(\"data/year_osward_grocery.csv\")\n",
    "display(year_grocery.head())\n",
    "\n",
    "wellbeing_grocery = pd.read_pickle(\"data/wellbeing_grocery.pkl\")\n",
    "\n",
    "grocery_analysis = pd.read_pickle(\"data/grocery_nutripoints.pkl\")\n",
    "display(grocery_analysis.head())\n",
    "print(grocery_analysis.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding labels and adding color\n",
    "\n",
    "In order to study better the repartition of these Nutritpoints, we will create two columns: one with the label corresponding to each cluster and another one with the corresponding color. \n",
    "\n",
    "We based our label and color assessment on the French Nutri-Score scale and rules."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's have a closer look at the nutrients associated with an unhealthy diet: salt, sugar and saturate:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grocery_analysis.groupby(\"nutri_class\").min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nutrilabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting nutri class (from 0 to 4) into letters (from A to E) called the nutrilabel\n",
    "def nutri_labels_to_letter(row_list):\n",
    "    if row_list == 0:\n",
    "        return \"A\"\n",
    "    elif row_list == 1:\n",
    "        return \"B\"\n",
    "    elif row_list == 2:\n",
    "        return \"C\"\n",
    "    elif row_list == 3:\n",
    "        return \"D\"\n",
    "    elif row_list == 4:\n",
    "        return \"E\"\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "grocery_analysis[\"nutrilabel\"] = grocery_analysis.apply(\n",
    "    lambda row: nutri_labels_to_letter(row[\"nutri_class\"]), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nutricolor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to set a color to each label\n",
    "def addcolor(row_list):\n",
    "    if row_list == \"A\":\n",
    "        return \"#038141\"\n",
    "    elif row_list == \"B\":\n",
    "        return \"#85BB2F\"\n",
    "    elif row_list == \"C\":\n",
    "        return \"#FECC02\"\n",
    "    elif row_list == \"D\":\n",
    "        return \"#EE8300\"\n",
    "    elif row_list == \"E\":\n",
    "        return \"#E63F11\"\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "grocery_analysis[\"color\"] = grocery_analysis.apply(\n",
    "    lambda row: addcolor(row[\"nutrilabel\"]), axis=1)\n",
    "display(grocery_analysis.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploratory data analysis\n",
    "\n",
    "We now try to understand how the nutripoints are distributed. We also want to know the density of each nutrilabel.\n",
    "Moreover, we thought it was important to visualise the correlation between the Nutripoints and each energy as we based our calculations on nutrients weight. The scatter plots is another way to observe the impact of the energy on Nutripoints. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Nutripoints distribution  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explanatory data analysis: distribution of the different variables by counting the number of observations\n",
    "plt.figure(figsize=(15, 8))\n",
    "\n",
    "sns.distplot(grocery_analysis[\"nutripoints\"])\n",
    "plt.xlabel('nutripoints')\n",
    "plt.ylabel('density')\n",
    "plt.title('nutripoints distribution')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# checking if nutripoints follow a normal distribution\n",
    "print(diagnostic.kstest_normal(grocery_analysis.nutripoints, dist='norm'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the p_value < 0.05 -> we can reject the null hypothesis that the nutripoints comes from a normal distribution! We could actually observe it on the plot as the curve was not very smooth. \n",
    "\n",
    "We can observe that most of the values are between 4.5 and 12. There is not a lot of variance between our different average products. This explains why we are not able to observe precise clusters. Indeed, we force a bit the separation of the areas in different clusters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Energy_total is the most correlated to nutripoints with an R value of 0.9. It is followed by carb and sugar. A potent explanation could be that the C points in our Nutripoints computation, corresponding to the \"bad\" points have more weight than the good ones. Indeed, those values are higher, there is always more fat and carbs than there is protein and fibre. \n",
    "\n",
    "Furthermore, we can see that entropy is negatively correlated with Nutripoints. Therefore, an average product with equal proportion of each nutrient leads to good Nutripoints. This is explained by the fact that Nutripoints value fibre and protein and on the contrary when there is too much saturate, sugar or salt, it is badly classified. As a good entropy rhymes with the same level of each nutrient: fiber, protein, fat and carb, the points C will be close to points A, leading to small Nutripoints and a good label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Nutrilabels distribution \n",
    "\n",
    "In order to know, which class has the more area, we plot the nutrilabel distribution. Indeed, it could help later to know if well-being variables play a role in that distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot of the nutrilabel density distribution\n",
    "plt.figure(figsize=(6, 4))\n",
    "\n",
    "plt.hist(x=grocery_analysis[\"nutrilabel\"])\n",
    "\n",
    "plt.xlabel('nutrilabel')\n",
    "plt.ylabel('density')\n",
    "plt.title('Nutrilabel Distribution')\n",
    "\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The boundary labels are the ones with fewer area. Indeed, most areas are aggregated and spread between B, C and D. C is the one with the most areas. It is actually quite bad because it means that most areas have \"unhealthy\" average product with mainly salt, sugar and/or fat. If we sum A and B on one hand and C, D and E on the other side, we can observe that the second sum is higher than A+B. Therefore, most of the average products are in the less healthy classes. However, we should not forget our Nutripoints distribution where we could observe that there was no big variance between the Nutripoints. Therefore, the boundaries between each label are not very clear and the difference between labels is actually very small. Assessing that C is not healthy whereas B is actually criticable,  we can only say that B products have mainly a better nutrition value than C products."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Merging to have the Index Score\n",
    "\n",
    "We add to our original database `wellbeing_nutripoints` a column with the Total Wellbeing Score per area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wellbeing_nutripoints = pd.merge(left=grocery_analysis, right=wellbeing_grocery[['area_id', 'Index Score 2013']],\n",
    "                                 on=\"area_id\")\n",
    "wellbeing_nutripoints.to_pickle(\"plot_data/plot_wellbeing_nutripoints.pkl\")\n",
    "display(wellbeing_nutripoints.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the Wellbeing Total Score in function of the nutripoints\n",
    "plt.figure(figsize=(14, 3))\n",
    "\n",
    "plt.scatter(wellbeing_nutripoints[\"nutripoints\"],\n",
    "            wellbeing_nutripoints[\"Index Score 2013\"], c=wellbeing_nutripoints[\"color\"])\n",
    "\n",
    "plt.xlabel(\"Nutripoints\")\n",
    "plt.ylabel(\"Well-being Index Score 2013\")\n",
    "plt.title(\"Well-being Total Score in funtion of the nutripoints\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above plot, we can better visualize the repartition of the nutrilabels. However, we cannot observe distinct clusters. Indeed, Nutripoint seems to follow a continuous distribution as almost values in the range of 1 to 15 have is linked to a Well-being Score.\n",
    "\n",
    "This plot confirms that nutrition is not well correlated to well-being. Indeed, well-being index score is not decreasing with the Nutripoints. Some points in B have a very low Well-being Score while a point in E has high Index Score. In each nutrilabel, we have all sort of Index Score. Therefore, we cannot make conclusions on the impact of well-being on nutritional variables.\n",
    "\n",
    "With the merging, we lost our outliers points from A label because we did not have their well-being value. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nutrients energy and nutripoints correlation and influence\n",
    "\n",
    "As we have seen, it is difficult to understand the effect of well-being on nutrition. We however want to observe the correlation between the nutrient energy and the Nutripoints as each nutrient play a role in the computation of this variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing the correlation between the different variables from the dataset according to the spearman method\n",
    "correlation = grocery_analysis.corr(method=\"spearman\")\n",
    "display(correlation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing and plotting the correlation between the different nutrients and the nutripoints\n",
    "plt.figure(figsize=(14, 3))\n",
    "correlation[\"nutripoints\"].plot.bar(x=None, y=None, width=0.8, legend=None)\n",
    "plt.ylabel(\"Spearman R\")\n",
    "plt.title(\"Correlation nutripoints\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Energy_total is the most correlated to nutripoints with an R value of 0.9. It is followed by carb and sugar. A potent explanation could be that the C points in our Nutripoints computation, corresponding to the \"bad\" points have more weight than the good ones. Indeed, those values are higher, there is always more fat and carbs than there is protein and fibre. \n",
    "\n",
    "Furthermore, we can see that entropy is negatively correlated with Nutripoints. Therefore, an average product with equal proportion of each nutrient leads to good Nutripoints. This is explained by the fact that Nutripoints value fibre and protein and on the contrary when there is too much saturate, sugar or salt, it is badly classified. As a good entropy rhymes with the same level of each nutrient: fiber, protein, fat and carb, the points C will be close to points A, leading to small Nutripoints and a good label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Representation of ward areas according to their values for food-related predictor pairs\n",
    "# Colors are set according to the nutrilabel of the area\n",
    "\n",
    "fig, ax = plt.subplots(3, 4, figsize=(16, 8), sharey=False)\n",
    "\n",
    "for i in range(1, 11):\n",
    "    sbplt = ax[int(i/4), i % 4]\n",
    "    sbplt.scatter(\n",
    "        grocery_analysis.iloc[:, i], grocery_analysis[\"h_nutrients_calories\"], c=grocery_analysis[\"color\"])\n",
    "    sbplt.set_xlabel('')\n",
    "    sbplt.set_ylabel('')\n",
    "    sbplt.set_title(grocery_analysis.columns[i], wrap=True)\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.subplots_adjust(top=0.9)\n",
    "\n",
    "fig.suptitle(\n",
    "    'Scatter plots of the different areas for food-related predictor pairs', fontsize=18);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When representing entropy in function of each nutrient or nutripoints, we can observe their impact on this variable.\n",
    "\n",
    "For instance, a low energy product would have a higher entropy than one with high energy. Indeed, high energy often means too much fat or carb which are very caloric taking place for other nutrients. It seems that most of A and B have lower total energy and therefore higher entropy. This decreasing trend can be observed for sugar and carb too but it is less distinct for fat and saturate. \n",
    "\n",
    "Moreover, increasing trend is more difficult to assess for protein and fibre. They are more aggregated in a corner of the graph.\n",
    "\n",
    "When observing the Nutripoints figure, we can assess that indeed, the lowest entropy are part of D and E classes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Typical Londoner product analysis \n",
    "\n",
    "We want to analyse the repartition of the different nutrients in an average product. \n",
    "\n",
    "We compute therefore the average total_weight and average weight of each nutrients. We then represent them in a pie chart to visualize it better. \n",
    "\n",
    "We will try to compare this repartition with the reference intakes recommendation set by the European law. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main classes of nutrients\n",
    "NUTRIENTS = [\"fibre\", \"protein\", \"carb\", \"fat\", \"salt\"]\n",
    "SUBNUTRIENTS = [\"sugar\", \"saturate\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dataframe with all the nutrient's weight from year_grocery\n",
    "weight = year_grocery[[\"area_id\"]+NUTRIENTS].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing the total nutrient's weight\n",
    "weight[\"weight_total\"] = weight[NUTRIENTS].sum(axis=1)\n",
    "display(weight.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding saturate and sugar values after computing the total weight since they are subcategories of nutrients\n",
    "weight[['sugar', 'saturate']] = year_grocery[[\"sugar\"] + [\"saturate\"]]\n",
    "weight['carb_not_sugar'] = year_grocery[\"carb\"]-year_grocery[\"sugar\"]\n",
    "weight['fat_not_saturate'] = year_grocery[\"fat\"]-year_grocery[\"saturate\"]\n",
    "weight['not_saturate_sugar_salt'] = weight[\"weight_total\"] - \\\n",
    "    year_grocery[\"saturate\"]-year_grocery[\"sugar\"]-year_grocery[\"salt\"]\n",
    "display(weight.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing the features for typical londonner product based on the average weight of each nutrients\n",
    "weight_mean = weight.mean(axis=0)\n",
    "display(weight_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pie chart, where the slices will be ordered and plotted counter-clockwise:\n",
    "nutrients_labels = [\"Fiber\", \"Protein\", \"Carbohydrates\", \"Fat\", \"Salt\"]\n",
    "colors = [\"limegreen\", \"red\", \"skyblue\", \"gold\", \"royalblue\"]\n",
    "fig1, ax1 = plt.subplots(figsize=(10, 8))\n",
    "ax1.pie(weight_mean[0:5], radius=1, labels=nutrients_labels,\n",
    "        autopct='%1.1f%%', startangle=90, colors=colors, textprops={'fontsize': 14}, pctdistance=0.85)\n",
    "ax1.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n",
    "\n",
    "plt.title('Proportion of nutrients in the average product',\n",
    "          fontsize=22, y=1.05, fontweight=\"bold\")\n",
    "plt.tight_layout(pad=0)\n",
    "path = '../xavoliva6.github.io/img'\n",
    "plt.savefig(f\"{path}/proportion_nutrients_average_london.svg\", format='svg',\n",
    "            transparent=True, bbox_inches='tight', pad_inches=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This pie chart visually shows the proportion of each nutrient in the average product in London. It will be compared to the recommendations for a healthy diet given by the [European law](https://www.nutrition.org.uk/attachments/article/234/Nutrition%20Requirements_Revised%20Oct%202016.pdf.)\n",
    "\n",
    "Recommendations:   \n",
    "Fats: 14%  \n",
    "Fibres: 14%  \n",
    "Carbs: 52%  \n",
    "Protein: 10%  \n",
    "Sodium: 1.2%\n",
    "Sugar: not more than 5%   \n",
    "Saturate: not more than 11%   \n",
    "\n",
    "By comparing the recommendations with the average product values, we find out that people in London eat too much fat and proteins. Moreover, the proportion of fibre in the Londoner diet is well below the recommendations. What about sugar, salt and saturate?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's have a closer look at the nutrients associated with an unhealthy diet: salt, sugar and saturate:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_sugar_saturate_salt = weight_mean[[\n",
    "    \"saturate\", \"sugar\", \"salt\", \"not_saturate_sugar_salt\"]]\n",
    "\n",
    "colors = [\"gold\", \"sienna\", \"royalblue\", \"silver\"]\n",
    "\n",
    "subnutrients_labels = [\"Saturate fats\", \"Sugar\", \"Salt\", \"Others\"]\n",
    "fig1, ax1 = plt.subplots(figsize=(10, 8))\n",
    "ax1.pie(weight_sugar_saturate_salt, radius=1, labels=subnutrients_labels,\n",
    "        autopct='%1.1f%%', startangle=90, colors=colors, textprops={'fontsize': 14}, pctdistance=0.7)\n",
    "ax1.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n",
    "\n",
    "plt.title(\n",
    "    'Proportion of negative nutrients in the average product', fontsize=22, y=1.05, fontweight=\"bold\")\n",
    "\n",
    "plt.savefig(f\"{path}/proportion_salt_sugar_saturate_average_london.svg\", format='svg',\n",
    "            transparent=True, bbox_inches='tight', pad_inches=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually perceived as \"bad\" nutrients, salt and sugar are consumed excessively in the total proportion of intakes. On the other hand, saturate, also classified as an unhealthy nutrient is consumed reasonably with a value that doesn't exceed the recommendations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true"
   },
   "source": [
    "### B) House Prices Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUTRITION_COLS = [\"area_id\", \"energy_tot\", \"energy_fat\", \"energy_saturate\", \"energy_sugar\", \"energy_protein\",\n",
    "                  \"energy_carb\", \"energy_fibre\", \"energy_alcohol\", \"h_nutrients_calories\"]\n",
    "\n",
    "year_grocery = pd.read_csv(\"data/year_lsoa_grocery.csv\")\n",
    "print(year_grocery.shape)\n",
    "display(year_grocery.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_grocery = year_grocery[['energy_tot', 'saturate', 'salt',\n",
    "                            'sugar', 'f_fruit_veg', 'fibre', 'protein']].min(axis=0)\n",
    "\n",
    "max_grocery = year_grocery[['energy_tot', 'saturate', 'salt',\n",
    "                            'sugar', 'f_fruit_veg', 'fibre', 'protein']].max(axis=0)\n",
    "\n",
    "year_grocery[\"nutripoints\"] = year_grocery.apply(\n",
    "    lambda row: calculate_nutripoints(row, min_grocery, max_grocery), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLS = [\"Code\", \"Year ending Dec 2014\"]\n",
    "\n",
    "housing_prices = pd.read_excel(\n",
    "    \"data/land-registry-house-prices-LSOA.xls\", sheet_name=\"Mean\")[COLS]\n",
    "housing_prices.rename(\n",
    "    columns={\"Year ending Dec 2014\": \"mean house price\"}, inplace=True)\n",
    "housing_prices.dropna(inplace=True)\n",
    "\n",
    "print(housing_prices.shape)\n",
    "display(housing_prices.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grocery_housing = pd.merge(\n",
    "    year_grocery, housing_prices, left_on=\"area_id\", right_on=\"Code\")\n",
    "grocery_housing.drop(\"Code\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grocery_housing['mean house price'] = pd.to_numeric(\n",
    "    grocery_housing['mean house price'], errors='coerce')\n",
    "display(grocery_housing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.histplot(data=grocery_housing, x=\"mean house price\")\n",
    "\n",
    "print(grocery_housing[\"mean house price\"].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "sns.ecdfplot(data=grocery_housing, x=\"mean house price\",\n",
    "             complementary=True, ax=ax[0])\n",
    "sns.ecdfplot(data=grocery_housing, x=\"mean house price\",\n",
    "             complementary=True, ax=ax[1])\n",
    "ax[1].set(xscale=\"log\", yscale=\"log\")\n",
    "\n",
    "\n",
    "price_mean = grocery_housing[\"mean house price\"].mean()\n",
    "price_median = grocery_housing[\"mean house price\"].median()\n",
    "\n",
    "print(price_mean, price_median, price_mean / price_median)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is skewed to the right, with a long tail of high scores pulling the mean up more than the median. According to the graphs, the data clearly does not follow a normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLS = [\"energy_tot\", \"energy_fat\", \"energy_saturate\", \"energy_sugar\", \"energy_protein\",\n",
    "        \"energy_carb\", \"energy_fibre\", \"energy_alcohol\", \"h_nutrients_calories\", \"nutripoints\"]\n",
    "\n",
    "correlation = grocery_housing.corr(method=\"spearman\").loc[COLS]\n",
    "\n",
    "correlation['mean house price'].plot.bar(\n",
    "    x=None, y=None, width=0.8, legend=None)\n",
    "\n",
    "plt.title(\"Mean house price\")\n",
    "plt.ylabel(\"spearman correlation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLS_CORRELATED = [\"energy_tot\", \"energy_sugar\", \"energy_carb\", \"energy_fibre\", \"h_nutrients_calories\", \"nutripoints\"]\n",
    "colors = [\"r\", \"r\", \"r\", \"g\", \"g\", \"r\"]\n",
    "\n",
    "N = len(COLS_CORRELATED)\n",
    "\n",
    "fig, ax = plt.subplots(2, 3, figsize=(25, 10))\n",
    "\n",
    "for i, col in enumerate(COLS_CORRELATED):\n",
    "    ax[int(i / 3), i % 3].scatter(grocery_housing[\"mean house price\"],\n",
    "                    y=grocery_housing[col], c=[colors[i]] * len(grocery_housing))\n",
    "    ax[int(i / 3), i % 3].set(xscale=\"log\", title=col, xlabel=\"mean house price\")\n",
    "    \n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nutripoints are negatively related to mean house prices. The graph on the ri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "house_prices_median = grocery_housing[\"mean house price\"].median()\n",
    "\n",
    "\n",
    "def classify_median(row, median):\n",
    "    if row[\"mean house price\"] > median:\n",
    "        return \"high\"\n",
    "    else:\n",
    "        return \"low\"\n",
    "\n",
    "\n",
    "grocery_housing[\"pricing\"] = grocery_housing.apply(\n",
    "    lambda i: classify_median(i, house_prices_median), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grocery_housing[['nutripoints', 'pricing']\n",
    "                ].boxplot(by='pricing', figsize=(10, 5))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predictive Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "grocery_housing.dropna(axis=0, inplace=True)\n",
    "grocery_housing_stand = grocery_housing.copy()\n",
    "grocery_housing_stand[[\"mean house price\", \"nutripoints\"]] = scaler.fit_transform(\n",
    "    grocery_housing[[\"mean house price\", \"nutripoints\"]])\n",
    "\n",
    "X = grocery_housing_stand[[\"mean house price\"]]\n",
    "y = grocery_housing_stand[\"nutripoints\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the models\n",
    "\n",
    "lin_reg = LinearRegression()\n",
    "gb_boost_reg = GradientBoostingRegressor(learning_rate=0.1, n_estimators=100)\n",
    "ridge_reg = Ridge(alpha=.5)\n",
    "mlp_reg = MLPRegressor(solver='lbfgs', alpha=1e-5,\n",
    "                       hidden_layer_sizes=(5, 2))\n",
    "dt_reg = DecisionTreeRegressor()\n",
    "\n",
    "mse_scores = {}\n",
    "\n",
    "reg_models = [lin_reg, gb_boost_reg, ridge_reg, mlp_reg, dt_reg]\n",
    "\n",
    "mse = 'neg_mean_squared_error'\n",
    "\n",
    "for model in reg_models:\n",
    "    model_scores = cross_validate(model, X, y, cv=5, scoring=[mse])\n",
    "\n",
    "    mse_scores[type(model).__name__] = model_scores[\"test_\" + mse]\n",
    "\n",
    "pp.pprint(mse_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_scores_df = pd.DataFrame(mse_scores).abs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = mse_scores_df.plot.bar(figsize=(15, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = mse_scores_df.mean(axis=0).idxmin()\n",
    "\n",
    "print(best_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C) Fast-Food Chains Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have studied if the nutritional properties of the average product per area were correlated to the well-being features, we now want to study the correlation between the nutrients composition and the number of fast-food restaurants per area.\n",
    "\n",
    "We were able to find the database from the [London Datastore](https://data.london.gov.uk/), the same one as for the well-being features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fast_food = pd.read_excel(\n",
    "    \"data/fast_food_ward.xlsx\", sheet_name=\"Ward Data\", header=[3], usecols=\"E,G\")\n",
    "display(fast_food.head())\n",
    "print(fast_food.shape)\n",
    "\n",
    "wellbeing_grocery = pd.read_pickle(\"data/wellbeing_grocery.pkl\")\n",
    "display(wellbeing_grocery.head())\n",
    "print(wellbeing_grocery.shape)\n",
    "\n",
    "grocery_analysis = pd.read_pickle(\"data/grocery_nutripoints.pkl\")\n",
    "display(grocery_analysis.head())\n",
    "print(grocery_analysis.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Merging\n",
    "\n",
    "We compare the number of lines of grocery dataset and fast-food one and try to see how many they have in common. We then merge the two datasets. We also check that there is no null or NA values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if area id is a unique id\n",
    "is_area_unique = grocery_analysis[\"area_id\"].is_unique\n",
    "print(is_area_unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nr_wards_grocery = len(set(grocery_analysis[\"area_id\"].values))\n",
    "print(nr_wards_grocery)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grocery_analysis.isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nr_wards_fast_food = len(set(fast_food[\"2015 Ward code\"].values))\n",
    "print(nr_wards_fast_food)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fast_food.isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fast_food.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate number of rows both datasets have in common\n",
    "\n",
    "nr_wards = len(set(fast_food[\"2015 Ward code\"].values)\n",
    "    & set(grocery_analysis[\"area_id\"].values))\n",
    "\n",
    "print(nr_wards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We loose 70 values by merging.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fastfood_grocery = pd.merge(\n",
    "    left=grocery_analysis, right=fast_food, left_on='area_id', right_on=\"2015 Ward code\")\n",
    "fastfood_grocery = fastfood_grocery.drop(\"2015 Ward code\", axis=1)\n",
    "display(fastfood_grocery.head())\n",
    "print(fastfood_grocery.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also merge well-being and fast-food in order to check the correlation between the well-being features and the number of fast-food, as we can make some assumptions about how they are linked and maybe prove the reliability of our data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fastfood_wellbeing = pd.merge(\n",
    "    left=wellbeing_grocery, right=fast_food, left_on='area_id', right_on=\"2015 Ward code\")\n",
    "fastfood_wellbeing = fastfood_wellbeing.drop(\"2015 Ward code\", axis=1)\n",
    "display(fastfood_wellbeing.head())\n",
    "print(fastfood_wellbeing.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We lose 33 values between well-being and grocery because wellbeing_grocery had already been merged before with year_grocery, leaving aside the area for which we did not have the well-being values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comprehension of the data\n",
    "\n",
    "##### Distribution of the values: describe, boxplot, distplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Understanding better how the values are distributed\n",
    "fastfood_grocery.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(4, 5))\n",
    "\n",
    "sns.boxplot(y=fastfood_grocery[\"Count of outlets\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They are many outliers for the number of outlets going up to 147 whereas the median is at 11. Due to this phenomenom, the mean is quite high whereas 75% of the values are between 1 and 18. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of columns of interest in the nutritional dataset\n",
    "COLUMNS_GROCERY = [\n",
    "    'energy_fat',\n",
    "    'energy_saturate',\n",
    "    'energy_sugar',\n",
    "    'energy_protein',\n",
    "    'energy_carb',\n",
    "    'energy_fibre',\n",
    "    'energy_alcohol',\n",
    "    'energy_tot',\n",
    "    'h_nutrients_calories',\n",
    "    'nutripoints',\n",
    "    'Count of outlets'\n",
    "]\n",
    "\n",
    "# Selection of the numerical columns of interest in the fastfood_grocery dataset\n",
    "fastfood_grocery_analysis = fastfood_grocery[COLUMNS_GROCERY].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(4, 3, figsize=(16, 8), sharey=False)\n",
    "\n",
    "for i in range(len(COLUMNS_GROCERY)):\n",
    "    sbplt = ax[int(i/3), i % 3]\n",
    "\n",
    "    sns.histplot(data=fastfood_grocery_analysis.iloc[:, i], ax=sbplt)\n",
    "    sbplt.set_xlabel('')\n",
    "    sbplt.set_ylabel('')\n",
    "    sbplt.set_title(fastfood_grocery_analysis.columns[i], wrap=True)\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.subplots_adjust(top=0.9)\n",
    "\n",
    "fig.suptitle('Histplot for each column', fontsize=18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the nutritional variables seem to be normally distributed. On the other side, the number of outlets is more logarithmic. Most of the areas have between 5 and 10 fast foods but it goes up to 140! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Correlation between the number of fast foods and the different nutritional variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap to visualize the correlation between the variables\n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(fastfood_grocery_analysis.corr())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The correlation with the number of oulets (the last column or row) seems really low as the colours are mainly red, corresponding to values around 0. We will display the correlation table for the count of outlets to better understand the importance of the correlation with the nutritional variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation = fastfood_grocery_analysis.corr(method=\"spearman\")\n",
    "display(correlation[[\"Count of outlets\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 3))\n",
    "correlation[\"Count of outlets\"].plot.bar(\n",
    "    x=None, y=None, width=0.8, legend=None)\n",
    "plt.ylabel(\"Spearman R\")\n",
    "plt.title(\"Correlation fast food outlets\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The number of fast food is not correlated at all with the Nutripoints.**   \n",
    "Furthermore, there is almost no correlation with other variables. Therefore, it is hardly justifiable to predict the nutritional informations of the average product of an area from the number of fast-food.\n",
    "\n",
    "We will however try to further investigate this correlation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Correlation between the number of fast food and the well-being measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of columns of interest in the wellbeing dataset\n",
    "COLUMNS_SCORES = [\n",
    "    'Life Expectancy',\n",
    "    'Childhood Obesity',\n",
    "    'Incapacity Benefit rate',\n",
    "    'Unemployment rate',\n",
    "    'Crime rate - Index',\n",
    "    'Deliberate Fires',\n",
    "    'Average Capped GCSE and Equivalent Point Score Per Pupil',\n",
    "    'Unauthorised Absence in All Schools (%)',\n",
    "    'Dependent children in out-of-work families',\n",
    "    'Public Transport Accessibility',\n",
    "    'Homes with access to open space & nature, and % greenspace',\n",
    "    'Subjective well-being average score',\n",
    "    'Index Score 2013',\n",
    "    'nutripoints',\n",
    "    'Count of outlets'\n",
    "]\n",
    "\n",
    "# Selection of the numerical columns of interest in the wellbeing_grocery dataset\n",
    "fastfood_wellbeing_analysis = fastfood_wellbeing[COLUMNS_SCORES].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation = fastfood_wellbeing_analysis.corr(method=\"spearman\")\n",
    "display(correlation[[\"Count of outlets\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the plot, we only keep the variables that have a spearman score higher than 0.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fastfood_wellbeing_analysis.drop(columns=['Incapacity Benefit rate',\n",
    "                                          'Deliberate Fires',\n",
    "                                          'Average Capped GCSE and Equivalent Point Score Per Pupil',\n",
    "                                          'Unauthorised Absence in All Schools (%)', 'Dependent children in out-of-work families',\n",
    "                                          'Homes with access to open space & nature, and % greenspace',\n",
    "                                          'Subjective well-being average score'], inplace=True)\n",
    "fastfood_wellbeing_analysis.to_pickle(\"plot_data/fastfood_wellbeing.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap to visualize the correlation between the variables\n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(fastfood_wellbeing_analysis.corr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_shorten = fastfood_wellbeing_analysis.corr(method=\"spearman\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 3))\n",
    "correlation_shorten.iloc[0:7, 7].plot.bar(\n",
    "    x=None, y=None, width=0.8, legend=None)\n",
    "plt.ylabel(\"Spearman R\")\n",
    "plt.title(\"Correlation fast food outlets-wellbeing\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When talking about fast-food and its relation with well-being variables, it is true that we already have some assumptions in our head. Of course, it is well correlated with Public Transport as to facilitate the access to the public and increase customer rate. On the other hand, Crime rate should be low (as we can see here it is negatively correlated to the number of fast-food) as the clients should feel safe when coming to the restaurants. However, we would have expected that it would be positively correlated with Childhood Obesity and Unemployment rate. We think that as fast-food are cheap, the chains won't target the fancy social class but mostly the ones that need comfort foods. Furthermore, fast-food don't serve healthy food but mostly fat and sugar riched one, causing obesity, so we thought that it was positively correlated with this variable. \n",
    "\n",
    "Therefore, some results seem predictable but others aren't. Thus, it is difficult to make a decisive conclusion about the link between fast-food and the Nutripoints of the average product of an area. We then do a regression analysis to see if it justifiable to try to predict Nutripoints from the number of fast-food in an area."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regression Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Linear Regression of Nutripoints from the number of fast-food"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Understanding the linear regression between the two variables\n",
    "Y = fastfood_grocery[[\"nutripoints\"]]\n",
    "X1 = fastfood_grocery[[\"Count of outlets\"]]\n",
    "X = sm.add_constant(X1)  # adding a constant\n",
    "\n",
    "model = sm.OLS(Y, X).fit()\n",
    "predictions = model.predict(X)\n",
    "\n",
    "print_model = model.summary()\n",
    "print(print_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The R-squared is very small: 0.005, meaning we can only predict 0.5% of our Nutripoints from the number of fast food. Furthermore, the coefficient of this latter is neglectable as it is -0.0137, the intercept is the one having the more weight in this regression. Therefore, we can conclude that the Linear Regression is not the adapted regression for prediction of Nutripoints of each area from the number of fast-food. It actually makes sense as there was no correlation between these two variables, it is actually difficult to predict one from the other. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Gradient Boosting Regressor\n",
    "\n",
    "We know that there is no correlation and that the Linear Regression is not representative at all. However, we still wanted to explore if another type of regression: the Gradient Boosting method, would be more concluent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train a gradient boosting regressor\n",
    "gradboost = GradientBoostingRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = fastfood_grocery[\"nutripoints\"]\n",
    "predicted_y = cross_val_predict(gradboost, X, y, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "ax.scatter(Y, predicted_y, edgecolors=(0, 0, 0))\n",
    "ax.set_xlabel('Original')\n",
    "ax.set_ylabel('Predicted')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the prediction was representative of the nutripoints, we should observe a diagonal line, it is not what we have at all. This means that for a certain Nutripoints value, the predicted one from the Gradient Boosting Regression is higher or smaller than the original value. \n",
    "\n",
    "To confirm this assumption, we compute the mean squared error and the R-squared value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2 = r2_score(Y, predicted_y)\n",
    "mse = mean_squared_error(Y, predicted_y)\n",
    "\n",
    "print(r2, mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The R-squared is negative. A horizontal line would actually be more representative of our regression. Furthermore, the mean squared error is very high. The regression is therefore not representative neither of our Nutripoints variable. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion \n",
    "\n",
    "As the correlation predicted it, it is not possible to find a good regression in order to predict the Nutripoints from the number of fast food of an area. Indeed, there are almost not correlated, making it difficult to have high coefficient for our regressions. \n",
    "\n",
    "Having the average products per area, actually, makes it difficult to have a representation of how healthy the inhabitants of an area consume. A high Nutripoint doesn't necessary mean that all the consumers of this area consume more fatty or sugary products. Furthermore, an unhealthy average product is not linked to a high number of fast-foods. However, it could be interesting to see if the richest areas have less fast-foods making the assumption that the inhabitants would prefer to go in a real restaurant.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
